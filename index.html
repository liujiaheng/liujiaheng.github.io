<!DOCTYPE html>
<html lang="en">
<head>
    <title>Jiaheng Liu</title>
    <!-- Meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="Xiaoying Riley at 3rd Wave Media">
    <link rel="shortcut icon" href="images/map_icon.png">

    <link href='https://fonts.googleapis.com/css?family=Lato:300,400,300italic,400italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Montserrat:400,700' rel='stylesheet' type='text/css'>

    <!-- FontAwesome JS -->
    <script defer src="assets/fontawesome/js/all.js"></script>

    <!-- Global CSS -->
    <link rel="stylesheet" href="assets/plugins/bootstrap/css/bootstrap.min.css">

    <!-- github calendar css -->
    <!-- <link rel="stylesheet" href="assets/plugins/github-calendar/dist/github-calendar-responsive.css"> -->
    <!-- github activity css -->
    <!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/octicons/2.0.2/octicons.min.css"> -->
    <!-- <link rel="stylesheet" href="assets/plugins/github-activity/src/github-activity.css"> -->

    <!-- Theme CSS -->
    <link id="theme-style" rel="stylesheet" href="assets/css/styles.css">

</head>

<body>
    <!-- ******HEADER****** -->
    <header class="header">
        <div class="container">
	        <div class="row align-items-center">
			    <div class="col">
		            <!-- <img class="profile-image img-fluid float-start rounded-circle" src="images/me2.jpg" alt="profile image" /> -->
                    <img class='profile-image img-fluid float-start rounded-circle' src="images/me.jpg" alt="profile image"
                        onmouseover="this.src='images/me.jpg'"
                        onmouseout="this.src='images/me.jpg'" />
		            <div class="profile-content">
		                <h1 class="name">Jiaheng Liu</h1>
		                <!-- <h2 class="desc">Web App Developer</h2> -->
		                <ul class="social list-inline">
		                    <!-- <li class="list-inline-item"><a href="https://twitter.com/xinntao" target="_blank"><i class="fab fa-twitter"></i></a></li> -->
		                    <!-- <li class="list-inline-item"><a href="#"><i class="fab fa-linkedin-in"></i></a></li> -->
                            <li class="list-inline-item"><a href="https://scholar.google.com/citations?user=yFI_RjUAAAAJ&hl=en" target="_blank"><i class="fab fa-google"></i></a></li>
		                    <li class="list-inline-item"><a href="https://github.com/liujiaheng" target="_blank"><i class="fab fa-github"></i></a></li>
		                    <li class="list-inline-item"><a href="email.html" target="_blank"><i class="fas fa-envelope-square"></i></a></li>
                            <li class="list-inline-item last-item"><a href="https://www.zhihu.com/people/liu-jia-heng-12" target="_blank"><i class="fab fa-zhihu"></i></a></li>
                                <!-- <div class="icon-with-text">
                                <i class="fa fa-fw"></i>
                                <span class="text">语雀</span>
                                </div></a></li> -->

		                </ul>
		            </div><!--//profile-->
			    </div><!--//col-->
	            <div class="col-12 col-md-auto">
		            <div class="dark-mode-switch d-flex">
						<div class="form-check form-switch mx-auto mx-md-0">
							<!-- <input type="checkbox" class="form-check-input me-2" id="darkSwitch" checked/> -->
                            <input type="checkbox" class="form-check-input me-2" id="darkSwitch"/>
							<label class="custom-control-label" for="darkSwitch">Dark Mode</label>
						</div>
			        </div><!--//dark-mode-switch-->
	                <a class="btn btn-cta-primary" href="email.html" target="_blank"><i class="fas fa-paper-plane"></i> Contact Me</a>
	            </div><!--//col-->
	        </div><!--//row-->
        </div><!--//container-->
    </header><!--//header-->

    <div class="container sections-wrapper py-5">
        <div class="row">
            <div class="primary col-lg-8 col-12">
                <section class="about section">
                    <div class="section-inner shadow-sm rounded">
                        <!-- <h2 class="heading">About Me</h2> -->
                        <div class="content">
				Currently, I am an <a href="https://is.nju.edu.cn/ljh/main.htm" target="_blank">Assistant Professor</a> at Nanjing University (2025-Present). Meanwhile, I am one of the founding members of <b>Multimodal Art Projection (M-A-P)</b>, which is an open-source research community on AIGC topics (e.g., text, audio, and vision modalities). 

Previously, I was a Research Scientist (<b>Alibaba Star Project</b>) at Alibaba (2023-2025), focusing on <b>Large Language Models (LLMs)</b>, including <b>pretraining, long-context modeling, reasoning, and alignment</b>.<br>
                            <font color="orange"><b>I  am actively looking for research interns and candidate students to work on LLM-related research topics</b></font>. <b>Please feel free to drop me an email to <font color="orange"><a href="liujiaheng@nju.edu.cn">liujiaheng@nju.edu.cn</a></font> if you are interested.</b><br>
                            </p>
                            <p align="left">
                                Moreover, I interned at SenseTime (working with Dr. Yichao Wu, Dr. Ding Liang), Baidu (working with Dr. Tan Yu), and Shanghai AI Lab (working with Dr. Tong He, Prof. Wanli Ouyang).
                                <!-- Previously, I was a senior staff researcher at<a href="https://arc.tencent.com/" target="_blank">Tencent ARC Lab</a> and <a href="https://ai.tencent.com/ailab/en/index/" target="_blank">Tencent AI Lab</a>, where I led an effort on <b>visual content generation (AIGC)</b>. <br> -->
                                I got my Ph.D. degree from Beihang University, advised by Prof. <a href="https://scse.buaa.edu.cn/info/1078/2655.htm" target="_blank">Ke Xu</a> and Prof. <a
                                href="https://scholar.google.com/citations?user=7Hdu5k4AAAAJ&hl=zh-CN" target="_blank">Dong Xu</a>.
                                Earlier, I obtained my bachelor's degree from Beihang University.
                            </p>
                        </div><!--//content-->
                    </div><!--//section-inner-->
                </section><!--//section-->

                <section class="about section">
                    <div class="section-inner shadow-sm rounded">
                        <!-- <h2 class="heading">About Me</h2> -->
                        <div class="content">
                            <p align="left">
                            I am currently working on <font color="orange"><b>Foundation Models</b></font>, which is a promising direction to AGI.<br>

                            <font color="orange"><b>● Large Language Models</b></font>
                            <ul>
                                <li><b>Pretraining</b>: MAP-Neo-7B, OpenCoder (ACL 2025), YuE, Sailor2, Chinese-Tiny-LLM (COLM 2024), D-CPT Law (NeurIPS 2024), E2-LLM (ACL 2024), MuPT (ICLR 2025), etc</li>
                                <li><b>Alignment</b>: RoleLLM (ACL 2024), Emulated Disalignment (ACL 2024, Outstanding Paper Award), 2D-DPO (NAACL 2025), DREAM (NAACL 2025), AIR (EMNLP 2025) etc</li>
                                <li><b>LLM Acceleration</b>: DDK (NeurIPS 2024), JSQ (ICML 2024), ACKD (ACL 2023), etc</li>
                                <li><b>Code Intelligence</b>: UniCoder (ACL 2024), R2C2-Coder, McEval (ICLR 2025), M2rcEval (ACL 2025), MdEval, CodeCriticBench, etc</li>
                                <li><b>Evaluation </b>: ConceptMath (ACL Findings 2024), MT-Bench-101 (ACL 2024), OWL (ICLR 2024), Chinese SimpleQA (ACL 2025), KORBench (ICLR 2025), MTU-Bench (ICLR 2025), SuperGPQA, DeltaBench (ACL 2025), etc</li>
                            </ul>
                            <font color="orange"><b>● Multimodal Large Language Models</b></font>
                            <ul>
                                <li><b>Pretraining</b>: MIO (EMNLP 2025), Mavors (ACMMM 2025)</li>
                                <li><b>Evaluation</b>: II-Bench (NeurIPS 2024), MMRA, OmniBench, LIME (ACL 2025), IV-Bench, etc</li>

                            </ul>
                            ● Previously, I worked on <b><font color="orange">Computer Vision</b></font>.
                            <ul>
                                <li><b>Face Recognition</b>: ICD-Face (ICCV 2023), OneFace (ECCV 2022), CoupleFace (ECCV 2022), AnchorFace (AAAI 2022), DAM (ICCV 2021), etc</li>
                                <li><b>Point Cloud Understanding</b>: LTA-PCS (CVPR 2024), SLF (ECCV 2024), VRDistill (ACMMM 2024), GD-MAE (CVPR 2023), APSNet (TIP 2022), GMT (TMM 2022), etc</li>
                                <li><b>Vision Model Acceleration </b>: BPNAS (TIP 2020), LAW (AAAI 2020), CCKD (ICCV 2019), RCO (ICCV 2019), etc</li>
                            </ul>
                        </div><!--//content-->
                    </div><!--//section-inner-->
                </section><!--//section-->

<!--=====================================================  News  ====================================================-->
                <section class="projects section">
                    <div class="section-inner shadow-sm rounded">
                        <h2 class="heading">News</h2>
                        <div class="item-content">
                            <ul class="resume-list" style="list-style: outside;list-style-type: square;">
																				<li> <b>[8/2025]</b> Six papers are accepted to NeurIPS 2025</li>
												<li> <b>[8/2025]</b> Three papers are accepted to EMNLP 2025.</li>
				<li> <b>[7/2025]</b> One paper is accepted to ACMMM 2025.</li>
				<li> <b>[5/2025]</b> Thirteen papers (10 Main and 3 Findings) are accepted to ACL 2025.</li>
				<li> <b>[1/2025]</b> Four papers are accepted to ICLR 2025.</li>
				<li> <b>[1/2025]</b> Two papers are accepted to NAACL 2025.</li>
                                <li> <b>[12/2024]</b> We will organize the  Open Science for Foundation Models Workshop in conjunction with <b>ICLR 2025</b>.</li>
                                <li> <b>[12/2024]</b> Two papers are accepted to AAAI 2025.</li>
                                <li> <b>[9/2024]</b> Four papers are accepted to NeurIPS 2024.</li>
                                <li> <b>[9/2024]</b> One paper is accepted to EMNLP 2024.</li>
                                <li> <b>[7/2024]</b> Three papers are accepted to ACMMM 2024.</li>
                                <!-- <li> <b>[09/2023]</b> Release <a href="https://github.com/TencentARC/T2I-Adapter" target="_blank">-Adapter</a> for SDXL: the most efficient control models, collaborating with <a href="https://huggingface.co/blog/t2i-sdxl-adapters" target="_blank">HuggingFace</a>.</li> -->
                                <li> <b>[7/2023]</b> One paper is accepted to CIKM 2024.</li>
                                <li> <b>[7/2024]</b> One paper is accepted to COLM 2024.</li>
                                <li> <b>[7/2024]</b> One paper is accepted to ECCV 2024.</li>
                                <li> <b>[5/2024]</b> Seven papers are accepted to ACL 2024.</li>
                                <li> <b>[5/2024]</b> One paper is accepted to ICML 2024.</li>
                                <li> <b>[2/2024]</b> One paper is accepted to CVPR 2024.</li>
                                <li> <b>[2/2024]</b> One paper is accepted to COLING 2024.</li>
                                <li> <b>[11/2023]</b> One paper is accepted to AAAI 2024.</li>
                                <li> <b>[2/2023]</b> One paper is accepted to CVPR 2023.</li>
                                <li> <b>[7/2023]</b> One paper is accepted to ICCV 2023.</li>
                                <li> <b>[9/2023]</b> One paper is accepted to EMNLP 2023.</li>
                                <li> <b>[5/2023]</b> One paper is accepted to ACL 2023.</li>



<!-- 
                                <li> <b>[10/2023]</b> Ranked as <a href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw/6" target="_blank">Top 2% Scientists Worldwide 2023</a> (Single Year) by Stanford University.</li>
                                <li> <b>[10/2023]</b> Two papers are accepted to NeurIPS 2023.</li>
                                <li> <b>[09/2023]</b> Release <a href="https://github.com/TencentARC/T2I-Adapter" target="_blank">T2I-Adapter</a> for SDXL: the most efficient control models, collaborating with <a href="https://huggingface.co/blog/t2i-sdxl-adapters" target="_blank">HuggingFace</a>.</li>
                                <li> <b>[07/2023]</b> Three papers are accepted to ICCV 2023.</li>
                                <li> <b>[04/2023]</b> One paper is accepted to ICML 2023.</li>
                                <li> <b>[03/2023]</b> We are holding the <a href="https://github.com/360SR/360SR-Challenge" target="_blank">360° Super-Resolution Challenge</a> as a part of the <a href="https://cvlai.net/ntire/2023/" target="_blank">NTIRE workshop</a> in conjunction with CVPR 2023.</li>
                                <li> <b>[02/2023]</b> Three papers to appear in CVPR 2023.</li>
                                <li> <b>[11/2022]</b> Two papers to appear in AAAI 2023.</li>
                                <li> <b>[09/2022]</b> Ranked as <a href="https://elsevier.digitalcommonsdata.com/datasets/btchxktzyw/5" target="_blank">Top 2% Scientists Worldwide 2022</a> (Single Year) by Stanford University.</li>
                                <li> <b>[09/2022]</b> Two papers to appear in NeurIPS 2022.</li>
                                <li> <b>[07/2022]</b> Two papers to appear in ECCV 2022. VQFR is accepted as <font color="orange">oral</font> (2.7%).</li>
                                <li> <b>[06/2022]</b> Two papers to appear in ACM MM 2022.</li>
                                <li> <b>[05/2022]</b> BasicSR joins the <a href="https://github.com/XPixelGroup" target="_blank">XPixel Group</a>!</li>
                                <li> <b>[04/2022]</b> We release a high-quality face video dataset (VFHQ). Please refer to the <a href="https://liangbinxie.github.io/projects/vfhq">project page</a> and <a href="https://arxiv.org/abs/2205.03409">our paper</a>.</li>
                                <li> <b>[12/2021]</b> One paper to appear in NeurIPS 2021 as <font color="orange">spotlight</font> (2.85%): <a href="https://proceedings.neurips.cc/paper/2021/hash/008bd5ad93b754d500338c253d9c1770-Abstract.html">FAIG: Finding Discriminative Filters for Specific Degradations in Blind Super-Resolution</a>. Codes are released in <a href="https://github.com/TencentARC/FAIG">TencentARC/FAIG</a>.</li>
                                <li> <b>[10/2021]</b> <a href="https://arxiv.org/abs/2107.10833">Real-ESRGAN</a> is accepted by ICCV 2021 AIM workshop with Honorary Nomination Paper Award.</li> -->
                                <!-- <details>
                                    <summary>Click for More</summary>
                                <li> [07/2021] One paper to appear in ICCV 2021: <a href="https://arxiv.org/abs/2108.08826">Towards Vivid and Diverse
                                    Image Colorization with Generative Color Prior</a>
                                </li>
                                <li> [07/2021] The codes for practical image restoration <a href="https://arxiv.org/abs/2107.10833">Real-ESRGAN</a>
                                    are released on <a href="https://github.com/xinntao/Real-ESRGAN">Github</a>.
                                </li>
                                <li> [06/2021] The training and testing codes of GFPGAN are released on <a
                                    href="https://github.com/TencentARC/GFPGAN">TencentARC</a>.
                                </li>
                                <li> [03/2021] 5 papers to appear in CVPR 2021.
                                </li>
                                <li> [03/2021] A brand-new <a href="https://github.com/xinntao/HandyView">HandyView</a> online!.
                                </li>
                                <li> [08/2020] A brand-new <a href="https://github.com/xinntao/BasicSR">BasicSR</a> v1.0.0 online!
                                </li>
                                <li> [06/2019] We have released the <a href="https://github.com/xinntao/EDVR">EDVR</a> training and testing codes
                                    and also updated <a href="https://github.com/xinntao/BasicSR">BasicSR</a> codes!
                                </li>
                                <li> [06/2019] Got my first outstanding reviewer recognition from CVPR 2019!
                                </li>
                                <li> [05/2019] Our video restoration method, <b>EDVR</b>, won all four tracks in the <a
                                    href="http://www.vision.ee.ethz.ch/ntire19/">NTIRE 2019 video restoration and enhancement challenges</a>.
                                    Check <a href="https://arxiv.org/abs/1905.02716">our paper</a> for more details.
                                </li>
                                <li> [03/2019] Our paper <a href="https://xinntao.github.io/projects/DNI"><i>Deep Network Interpolation for
                                        Continuous Imagery Effect Transition</i></a> to appear in CVPR 2019.
                                </li>
                                <li> [08/2018] Our SuperSR team won the third track of the <a href="https://www.pirm2018.org/PIRM-SR.html">2018
                                    PIRM Challenge on Perceptual Super-Resolution</a>. Check the report <a
                                    href="https://arxiv.org/abs/1809.00219"><i>ESRGAN</i></a> for more details.
                                </li>
                                <li> [06/2018] We won the <a href="http://www.vision.ee.ethz.ch/ntire17/NTIRE">NTIRE 2018 Challenge on Single Image
                                    Super-Resolution</a> as first runner-up and ranked the first in the <i>Realistic Wild ×4 conditions</i>
                                    track.
                                </li>
                                <li> [02/2018] Our paper <a href="http://mmlab.ie.cuhk.edu.hk/projects/SFTGAN/"><i>Recovering Realistic Texture in
                                        Image Super-resolution by Deep Spatial Feature Transform</i></a> to appear in CVPR 2018.
                                </li>
                                <li> [07/2017] Our HelloSR team won the <a href="http://www.vision.ee.ethz.ch/ntire17/NTIRE">NTIRE 2017 Challenge
                                    on Single Image Super-Resolution</a> as first runner-up.
                                </li>
                                </details> -->
                            </ul>
                        </div><!--//content-->
                    </div><!--//section-inner-->
                </section><!--//section-->


<!--=====================================================  Publications  ====================================================-->
               <section class="latest section">
                    <div class="section-inner shadow-sm rounded">
                        <h2 class="heading">Publications (Update Soon) <a href="https://scholar.google.com/citations?user=yFI_RjUAAAAJ&hl=en" target="_blank">[Full List]</a></h2>
                        <div class="content">
                                <small>(* equal contribution, <sup>#</sup> corresponding author)</small>
                        <br>
                        <ul class="resume-list" style="list-style: outside;list-style-type: square;">
                            <li>
                            PTSBench: A Comprehensive Post-Training Sparsity Benchmark Towards Algorithms and Models</br>
                                Zining Wang, Jinyang Guo, Ruihao Gong, Yang Yong, Aishan Liu, Yushi Huang, <b>Jiaheng Liu</b>, Xianglong Liu</br>
                                   ACM Multimedia (<b>ACM MM</b>), 2024 </br>
                                    <p></p>
                                </li>
                                                                <li>
                            MaterialSeg3D: Segmenting Dense Materials from 2D Priors for 3D Assets</br>
                                Zeyu Li, Ruitong Gan, Chuanchen Luo, Yuxi Wang, <b>Jiaheng Liu</b>, Ziwei Zhu, Man Zhang, Qing Li, Zhaoxiang Zhang, Junran Peng, Xu-Cheng Yin</br>
                            
                                   ACM Multimedia (<b>ACM MM</b>), 2024 </br>
                                    <p></p>
                                </li>
                                                            <li>
                                    VRDistill: Vote Refinement Distillation for Efficient Indoor 3D Object Detection
                            </br>
                                    
                            Ze Yuan, Jinyang Guo, Dakai An, Junran Wu, He Zhu, Jianhao Li, Xueyuan Chen, Ke Xu,  <b>Jiaheng Liu#</b></br>
                                   ACM Multimedia (<b>ACM MM</b>), 2024 </br>
                                    <p></p>
                                </li>
                                                        <li>
                                    Chinese Tiny LLM: Pretraining a Chinese-Centered Large Language Model
                            </br>
                                    
                            Xinrun Du, Zhouliang Yu, Songyang Gao, Ding Pan, Cheng Yuyang, Ziyang Ma, Ruibin Yuan, Xingwei Qu,  <b>Jiaheng Liu</b>, Tianyu Zheng, Xinchen Luo, Guorui Zhou, Binhang Yuan, Wenhu Chen, Jie Fu, Ge Zhang</br>
                                    Conference on Language Modeling (<b>COLM</b>), 2024 </br>
                                    <p></p>
                                </li>
                                                        <li>
                                    Segment, Lift and Fit: Automatic 3D Shape Labeling from 2D Prompts</br>
                                    
                            Jianhao Li, Tianyu Sun, Zhongdao Wang, Enze Xie, Bailan Feng, Hongbo Zhang, Ze Yuan, Ke Xu, <b>Jiaheng Liu#</b>, Ping Luo</br>
                                    European Conference on Computer Vision (<b>ECCV</b>), 2024 </br>
                                    <p></p>
                                </li>
                                     <li>
                                  Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!
                            </br>
                             Zhanhui Zhou, Jie Liu, Zhichen Dong, <b>Jiaheng Liu</b>, Chao Yang, Wanli Ouyang, Yu Qiao</br>
                                    Association for Computational Linguistics (<b>ACL</b>), 2024</br>
                                    <p></p>
                               </li>	
                                <li>
                                  UniCoder: Scaling Code Large Language Model via Universal Code
                            </br>
                                Tao Sun, Linzheng Chai, Jian Yang, Yuwei Yin, Hongcheng Guo,  <b>Jiaheng Liu</b>, Bing Wang, Liqun Yang, Zhoujun Li
                             </br>
                                     Association for Computational Linguistics (<b>ACL</b>), 2024</br>
                                    <p></p>
                               </li>	
                                 <li>
                                  Towards Real-world Scenario: Imbalanced New Intent Discovery
                            </br>
                            Shun Zhang, Chaoran Yan, Jian Yang, <b>Jiaheng Liu</b>, Ying Mo, Jiaqi Bai, Tongliang Li, Zhoujun Li</br>
                                     Association for Computational Linguistics (<b>ACL</b>), 2024</br>
                                    <p></p>
                               </li>	
                                     <li>
                                  MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues
                            </br>
                             Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, <b>Jiaheng Liu</b>, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, Wanli Ouyang</br>
                                     Association for Computational Linguistics (<b>ACL</b>), 2024</br>
                                    <p></p>
                               </li>	
                                 <li>
                                 E2-LLM: Efficient and Extreme Length Extension of Large Language Models
                            </br>
                             <b>Jiaheng Liu*</b>, Zhiqi Bai*, Yuanxing Zhang, Chenchen Zhang, Yu Zhang, Ge Zhang, Jiakai Wang, Haoran Que, Yukang Chen, Wenbo Su, Tiezheng Ge, Jie Fu, Wenhu Chen, Bo Zheng</br>
                                    Findings of Association for Computational Linguistics (<b>ACL</b>), 2024</br>
                                    <p></p>
                               </li>	
                                 <li>
                                  ConceptMath: A Bilingual Concept-wise Benchmark for Measuring Mathematical Reasoning of Large Language Models
                            </br>
                            Yanan Wu*, Jie Liu*, Xingyuan Bu*,<b>Jiaheng Liu#</b>, Zhanhui Zhou, Yuanxing Zhang, Chenchen Zhang, Zhiqi Bai, Haibin Chen, Tiezheng Ge, Wanli Ouyang, Wenbo Su, Bo Zheng</br>
                                    Findings of Association for Computational Linguistics (<b>ACL</b>), 2024</br>
                                    <p></p>
                               </li>	
                            
                            
                                 <li>
                                  RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models
                            </br>
                             Zekun Wang*, Zongyuan Peng*, Haoran Que*, <b>Jiaheng Liu#</b>, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Jian Yang, Man Zhang, Zhaoxiang Zhang, Wanli Ouyang, Ke Xu, Wenhao Huang, Wenhu Chen, Jie Fu, Junran Peng</br>
                                    Findings of Association for Computational Linguistics (<b>ACL</b>), 2024</br>
                                    <p></p>
                               </li>	
                                     <li>
                                  Compressing Large Language Models by Joint Sparsification and Quantization</br>
                                Jinyang Guo, Jianyu Wu, Zining Wang, <b>Jiaheng Liu</b>, Ge Yang, Yifu Ding, Ruihao Gong, Haotong Qin, Xianglong Liu</br>
                                    International Conference on Machine Learning (<b>ICML</b>), 2024</br>
                                    <p></p>
                               </li>
                                 <li>
                                   LTA-PCS: Learnable Task-Agnostic Point Cloud Sampling</br>
                             <b>Jiaheng Liu*</b>, Jianhao Li*, Jinyang Guo, Kaisiyuan Wang, Hongcheng Guo, Jian Yang, Junran Peng, Xianglong Liu, Ke Xu</br>
                                    IEEE Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024</br>
                                    <p></p>
                               </li>
                                    <li>
                                    m3P: Towards Multimodal Multilingual Translation with Multimodal Prompt</br>
                                Jian Yang, Hongcheng Guo, Yuwei Yin, Jiaqi Bai, Bing Wang, <b>Jiaheng Liu</b>, Xinnian Liang, LinZheng Chai, Liqun Yang, Zhoujun Li</br>
                                    Joint International Conference on Computational Linguistics, Language Resources and Evaluation (<b>COLING</b>), 2024</br>
                                    <p></p>
                                </li>
                                <li>
                                    OWL: A Large Language Model for IT Operations</br>
                                Hongcheng Guo, Jian Yang#, <b>Jiaheng Liu#</b>, Liqun Yang, Linzheng Chai, Jiaqi Bai, Junran Peng, Xiaorong Hu, Chao Chen, Dongfeng Zhang, Xu Shi, Tieqiao Zheng, Liangfan Zheng, Bo Zhang, Ke Xu, Zhoujun Li</br>
                                    International Conference on Learning Representations (<b>ICLR</b>), 2024</br>
                                    <p></p>
                                </li>	
                                <li>
                                    LogFormer: A Pre-train and Tuning Pipeline for Log Anomaly Detection</br>
                                Hongcheng Guo, Jian Yang, <b>Jiaheng Liu</b>, Jiaqi Bai, Boyang Wang, Zhoujun Li, Tieqiao Zheng, Bo Zhang, Junran Peng, Qi Tian</br>
                                    AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2024</br>
                                    <p></p>
                                </li>
                                <li>
                                    M2C: Towards Automatic Multimodal Manga Complement</br>
                                Hongcheng Guo, Boyang Wang, Jiaqi Bai, <b>Jiaheng Liu</b>, Jian Yang, Zhoujun Li</br>
                                    Findings of Empirical Methods in Natural Language Processing (<b>EMNLP</b>), 2023</br>
                                    <p></p>
                                </li>
                                    <li>
                                    ICD-Face: Intra-class Compactness Distillation for Face Recognition
                            </br>
                                   
                            Zhipeng Yu, <b>Jiaheng Liu#</b>, Haoyu Qin, Yichao Wu, Kun Hu, Jiayi Tian, Ding Liang</br>
                                    IEEE International Conference on Computer Vision (<b>ICCV</b>), 2023 </br>
                                    <p></p>
                                </li>
                                 <li>
                                  GripRank: Bridging the Gap between Retrieval and Generation via the Generative Knowledge Improved Passage Ranking</br>
                                Jiaqi Bai, Hongcheng Guo, <b>Jiaheng Liu</b>, Jian Yang, Xinnian Liang, Zhao Yan, Zhoujun Li</br>
                                    ACM International Conference on Information and Knowledge Management (<b>CIKM</b>), 2023</br>
                                    <p></p>
                               </li>
                             <li>
                                  Adaptive Contrastive Distillation for BERT Compression</br>
                             Jinyang Guo*, <b>Jiaheng Liu*</b>, Zining Wang, Yuqing Ma, Ruihao Gong, Ke Xu, Xianglong Liu</br>
                                    Findings of Association for Computational Linguistics (<b>ACL</b>), 2023</br>
                                    <p></p>
                               </li>	
                             <li>
                                   GD-MAE: Generative Decoder for MAE Pre-training on LiDAR Point Clouds</br>
                             Honghui Yang, Tong He, <b>Jiaheng Liu</b>, Hua Chen, Boxi Wu, Binbin Lin, Xiaofei He, Wanli Ouyang</br>
                                    IEEE Computer Vision and Pattern Recognition (<b>CVPR</b>), 2023</br>
                                    <p></p>
                               </li>
                                        <li>
                                    LogLG: Weakly Supervised Log Anomaly Detection via Log-Event Graph Construction</br>
                                Hongcheng Guo, Yuhui Guo, Jian Yang, <b>Jiaheng Liu#</b>, Zhoujun Li, Tieqiao Zheng, Liangfan Zheng, Weichao Hou, Bo Zhang</br>
                                    International Conference on Database Systems for Advanced Applications (<b>DASFAA</b>), 2023</br>
                                    <p></p>
                                </li>
                                    <li>
                                    LVP-M3: Language-aware Visual Prompt for Multilingual Multimodal Machine Translation</br>
                                Hongcheng Guo*, <b>Jiaheng Liu*</b>, Haoyang Huang, Jian Yang, Zhoujun Li, Dongdong Zhang, Zheng Cui</br>
                                    Empirical Methods in Natural Language Processing (<b>EMNLP</b>), 2022</br>
                                    <p></p>
                                </li>
                                <li>
                                    3D-Pruning: A Model Compression Framework for Efficient 3D Action Recognition</br>
                                    Jinyang Guo*, <b>Jiaheng Liu*</b>, Dong Xu</br>
                                    IEEE Transactions on Circuits and Systems for Video Technology (<b>TCSVT</b>), 2022</br>
                                    <p></p>
                                </li>
                            
                                <li>
                                   GeometryMotion-Transformer: An End-to-End Framework for 3D Action Recognition</br>
                                    <b>Jiaheng Liu</b>, Jinyang Guo, Dong Xu</br>
                                    IEEE Transactions on Multimedia (<b>TMM</b>), 2022</br>
                                    <p></p>
                                </li>
                                 <li>
                                    OneFace: One Threshold for All</br>
                                    <b>Jiaheng Liu</b>, Zhipeng Yu, Haoyu Qin, Yichao Wu, Ding Liang, Gangming Zhao, Ke Xu</br>
                                    European Conference on Computer Vision (<b>ECCV</b>), 2022 </br>
                                    <p></p>
                                </li>
                                                        <li>
                                    CoupleFace: Relation Matters for Face Recognition Distillation</br>
                                    <b>Jiaheng Liu</b>, Haoyu Qin, Yichao Wu, Jinyang Guo, Ding Liang, Ke Xu</br>
                                    European Conference on Computer Vision (<b>ECCV</b>), 2022 </br>
                                    <p></p>
                                </li>
                                    <li>
                                    APSNet: Towards Adaptive Point Sampling for Efficient 3D Action Recognition</br>
                                    <b>Jiaheng Liu*</b>, Jinyang Guo*, Dong Xu</br>
                                    IEEE Transactions on Image Processing (<b>TIP</b>), 2022 </br>
                                    <p></p>
                                </li>
                                        <li>
                                   Deep 3D Vessel Segmentation based on Cross Transformer Network
                            </br>
                                    Chengwei Pan, Baolian Qi, Gangming Zhao,  <b>Jiaheng Liu</b>, Chaowei Fang, Dingwen Zhang, Jinpeng Li</br>
                                    International Conference on Bioinformatics and Biomedicine (<b>BIBM</b>), 2022 </br>
                                    <p></p>
                                </li>
                                    <li>
                                   Computer-aided Tuberculosis Diagnosis with Attribute Reasoning Assistance
                            </br>
                                    Chengwei Pan, Gangming Zhao, Junjie Fang, Baolian Qi, <b>Jiaheng Liu</b>, Chaowei Fang, Dingwen Zhang, Jinpeng Li, Yizhou Yu</br>
                                    Medical Image Computing and Computer Assisted Interventions (<b>MICCAI</b>), 2022 </br>
                                    <p></p>
                                </li>
                                <li>
                                   Cross-Lingual Cross-Modal Consolidation for
                            Effective Multilingual Video Corpus Moment Retrieval</br>
                                    <b>Jiaheng Liu</b>, Tan Yu, Hanyu Peng, Mingming Sun, Ping Li</br>
                                    Findings of North American Chapter of the Association for Computational Linguistics (<b>NAACL</b>), 2022 </br>
                                    <p></p>
                                </li>
                                <li>
                                    AnchorFace: Boosting TAR@FAR for Practical Face Recognition</br>
                                    <b>Jiaheng Liu</b>, Haoyu Qin, Yichao Wu, Ding Liang</br>
                                    AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2022 </br>
                                    <p></p>
                                </li>
                            
                                <li>
                                    DAM: Discrepancy Alignment Metric for Face Recognition</br>
                                    <b>Jiaheng Liu</b>, Yudong Wu, Yichao Wu, Chuming Li, Xiaolin Hu, Ding Liang, Mengyu Wang</br>
                                    IEEE International Conference on Computer Vision (<b>ICCV</b>), 2021 </br>
                                    <p></p>
                                </li>
                            
                                <li>
                                    JointPruning: Pruning Networks along Multiple Dimensions for Efficient Point Cloud Processing</br>
                                    Jinyang Guo, <b>Jiaheng Liu</b>, Dong Xu</br>
                                    IEEE Transactions on Circuits and Systems for Video Technology (<b>TCSVT</b>), 2021</br>
                                    <p></p>
                                </li>
                            
                                <li>
                                    GeometryMotion-Net: A Strong Two-stream Baseline for 3D Action Recognition</br>
                                    <b>Jiaheng Liu</b>, Dong Xu</br>
                                    IEEE Transactions on Circuits and Systems for Video Technology (<b>TCSVT</b>), 2021</br>
                                    <p></p>
                                </li>
                            
                                <li>
                                    Block Proposal Neural Architecture Search</br>
                                    <b>Jiaheng Liu</b>, Shunfeng Zhou, Yichao Wu, Ken Chen, Wanli Ouyang, Dong Xu</br>
                                    IEEE Transactions on Image Processing (<b>TIP</b>), 2020 </br>
                                    <p></p>
                                </li>
                            
                                <li>
                                    Learning to Auto Weight: Entirely Data-driven and Highly Efficient Weighting Framework</br>
                                    Zhenmao Li, Yichao Wu, Ken Chen, Yudong Wu, Shunfeng Zhou, <b>Jiaheng Liu</b>, Junjie Yan </br>
                                    AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2020 </br>
                                    <p></p>
                                </li>
                                <li>
                                    Correlation Congruence for Knowledge Distillation</br>
                                    Baoyun Peng, Xiao Jin, <b>Jiaheng Liu</b>, Shunfeng Zhou, Yichao Wu, Yu Liu, Dongsheng Li, Zhaoning Zhang</br>
                                    IEEE International Conference on Computer Vision (<b>ICCV</b>), 2019 </br>
                                    <p></p>
                                </li>   
                                <li>
                                    Knowledge Distillation via Route Constrained Optimization</br>
                                    Xiao Jin, Baoyun Peng, Yichao Wu, Yu Liu, <b>Jiaheng Liu</b>, Ding Liang, Junjie Yan, Xiaolin Hu</br>
                                    IEEE International Conference on Computer Vision (<b>ICCV</b>), 2019 </br>
                                    <p></p>
                                </li>
                            <font color="#49ac43"><b>To be updated</b></font>
                        </ul>


                        </div><!--//content-->
                    </div><!--//section-inner-->
                </section><!--//section-->

            </div><!--//primary-->
            
            <div class="secondary col-lg-4 col-12">
                 <aside class="info aside section">
                    <div class="section-inner shadow-sm rounded">
                        <h2 class="heading sr-only">Basic Information</h2>
                        <div class="content">
                            <ul class="list-unstyled">
                                <li><i class="fas fa-envelope"></i><span class="sr-only">Email:</span><a href="email.html">liujiaheng@nju.edu.cn</a></li>
                                <li><i class="fab fa-google"></i><span class="sr-only">Google Scholar:</span><a href="https://scholar.google.com/citations?user=yFI_RjUAAAAJ&hl=en">Google Scholar</a></li>
                                <li><i class="fab fa-github"></i><span class="sr-only">GitHub:</span><a href="https://github.com/liujiaheng">GitHub</a></li>
                                <li><i class="fas fa-map-marker-alt"></i><span class="sr-only">Location:</span>Beijing, China</li>
                                <!-- <li><i class="fas fa-link"></i><span class="sr-only">Website:</span><a href="#">https://www.website.com</a></li> -->
                            </ul>
                        </div><!--//content-->
                    </div><!--//section-inner-->
                </aside><!--//aside-->

                <aside class="education aside section">
                    <div class="section-inner shadow-sm rounded">

                        <h2 class="heading">Education</h2>
                        <div class="content">
                            <div class="item">
                                <h3 class="title"><i class="fas fa-graduation-cap"></i> Ph.D. in Beihang University, <span class="year">2019-2023</span></h3>
                                <!-- <h4 class="university"><a href="http://mmlab.ie.cuhk.edu.hk/" target="_blank">Multimedia
              Laboratory (MMLab)</a>,<br>
            <a href="https://www.cuhk.edu.hk/english/index.html" target="_blank">Beihang University</a>, <span class="year">2016-2020</span></h4> -->
                            </div><!--//item-->
                            <div class="item">
                                <h3 class="title"><i class="fas fa-graduation-cap"></i> B.Eng. in Beihang University, <span class="year">2015-2019</span></h3>
                            </div><!--//item-->
                        </div><!--//content-->
                    </div><!--//section-inner-->
                </aside><!--//section-->

                
            </div><!--//secondary-->
        </div><!--//row-->
    </div><!--//masonry-->

    <!-- ******FOOTER****** -->
    <footer class="footer">
        <div class="container text-center">
                <!--/* This template is free as long as you keep the attribution link below. Thank you for your support. :) If you'd like to use the template without the attribution, you can buy the commercial license via our website: themes.3rdwavemedia.com */-->
                <small class="copyright">This template is modified from <a href="https://themes.3rdwavemedia.com/demo/bs5/developer/" target="_blank">Xiaoying Riley's project</a></small>
        </div><!--//container-->
    </footer><!--//footer-->

    <!-- Javascript -->
    <script type="text/javascript" src="assets/plugins/popper.min.js"></script>
    <script type="text/javascript" src="assets/plugins/bootstrap/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="assets/plugins/vanilla-rss/dist/rss.global.min.js"></script>
    <script type="text/javascript" src="assets/plugins/dark-mode-switch/dark-mode-switch.min.js"></script>
    <!-- custom js -->
    <script type="text/javascript" src="assets/js/main.js"></script>
</body>
</html>

